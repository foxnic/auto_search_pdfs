{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto_Search_PDF_Tool_Similar_Terms.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-JuVsmL8FiA0"
      },
      "source": [
        "# Auto Search PDF Tool With Additional Similar Search Terms\n",
        "\n",
        "(A Google Colab notebook.)\n",
        "\n",
        "1. Upload the PDFs you want to search\n",
        "2. Update the list of keywords (in the cell containing \"all_search_terms...\") to meet your needs\n",
        "3. Run the cells\n",
        "4. Once the following files appear in the pane to the left, you can download them by hovering over the filename, clicking on the three dots, then clicking 'download'\n",
        "\n",
        "- file_df.csv\n",
        "- hits_per_file.csv\n",
        "- keywords_by_file.csv\n",
        "- pages_by_file.csv"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nsNqV7y9Er68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28d2ded1-075e-4da6-a09d-073cb4e7d134"
      },
      "source": [
        "!pip install PyMuPDF"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: PyMuPDF in /usr/local/lib/python3.7/dist-packages (1.18.12)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6PtSvQz_3G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a3945b7-d99e-4d8a-d51b-ad5bbdaa6a2a"
      },
      "source": [
        "import spacy\n",
        "import spacy.cli\n",
        "spacy.cli.download(\"en_core_web_lg\")\n",
        "import en_core_web_lg\n",
        "nlp = en_core_web_lg.load()\n",
        "import numpy as np\n",
        "import fitz\n",
        "import nltk\n",
        "import os \n",
        "import pandas as pd\n",
        "\n",
        "# what files are in the working directory?\n",
        "# if you've not uploaded the PDFs then do that now!\n",
        "list_of_filenames = os.listdir()\n",
        "list_of_filenames"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[38;5;2mâœ” Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_lg')\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['.config',\n",
              " 'Will blockchain emerge as a tool to break the poverty chain in the Global South.pdf',\n",
              " '.ipynb_checkpoints',\n",
              " 'Technology Adoption Dynamics of the Press Workers in Bangladesh.pdf',\n",
              " 'sample_data']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5TY_qpUBB4_"
      },
      "source": [
        "all_search_terms = [\"technology\", \"internet\", \"smartphone\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNQl3igYRgEV"
      },
      "source": [
        "## Get similar search terms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "166yHMm5Rl82"
      },
      "source": [
        "# define a function to get the x most similar words to a word\n",
        "def most_similar(word, topn=2):\n",
        "    print(word)\n",
        "    word = nlp.vocab[str(word)]\n",
        "    print(word)\n",
        "    queries = [\n",
        "        w for w in word.vocab \n",
        "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
        "    ]\n",
        "\n",
        "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
        "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK2Hc0VpRtTl"
      },
      "source": [
        "# create function to receive a list of words and return the \n",
        "# top 2 similar words for each word in the list\n",
        "\n",
        "def get_similar_words(list_of_words):\n",
        "    \n",
        "    all_similar_words = []\n",
        "    \n",
        "    for word in list_of_words:\n",
        "        spacy_word = nlp.vocab[str(word)]\n",
        "        if spacy_word.has_vector:\n",
        "        \n",
        "            # find similar words to the word, and store them in a dataframe along with their scores\n",
        "            similar_words = pd.DataFrame(most_similar(word, topn=2), columns=[\"word\", \"similarity_score\"])\n",
        "\n",
        "            # save the list of similar words\n",
        "            similar_words_list = list(similar_words[\"word\"])\n",
        "\n",
        "            # append the list of similar words to the list to be returned\n",
        "            all_similar_words.append(similar_words_list)\n",
        "        \n",
        "    # flatten the list of lists to one list\n",
        "    all_similar_words = [item for sublist in all_similar_words for item in sublist]\n",
        "    \n",
        "    # remove duplicates from the list\n",
        "    all_similar_words = list(dict.fromkeys(all_similar_words))\n",
        "    \n",
        "    # sort list in alphabetical order\n",
        "    all_similar_words.sort()\n",
        "\n",
        "    return all_similar_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_5uVCgXR0zJ",
        "outputId": "3daac42e-463e-494c-e5b0-f5fe5c8d7014"
      },
      "source": [
        "# run the function on the search terms entered by the user\n",
        "new_search_terms = get_similar_words(all_search_terms)\n",
        "new_search_terms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "technology\n",
            "<spacy.lexeme.Lexeme object at 0x7f418c945eb0>\n",
            "internet\n",
            "<spacy.lexeme.Lexeme object at 0x7f418c9eafa0>\n",
            "smartphone\n",
            "<spacy.lexeme.Lexeme object at 0x7f418c7e7eb0>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['handset', 'online', 'smartphones', 'technological', 'technologies', 'web']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBVZDZMyRmOS",
        "outputId": "0a5767e9-b2cf-4a75-b8cf-3ef17092f0f8"
      },
      "source": [
        "all_search_terms = all_search_terms + new_search_terms\n",
        "all_search_terms"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['technology',\n",
              " 'internet',\n",
              " 'smartphone',\n",
              " 'handset',\n",
              " 'online',\n",
              " 'smartphones',\n",
              " 'technological',\n",
              " 'technologies',\n",
              " 'web']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T7361SwRitP"
      },
      "source": [
        "## Run the search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "togu7PAIAX9m"
      },
      "source": [
        "# for each pdf, read in the text to a new row in a dataframe\n",
        "df = pd.DataFrame(columns=[\"filename\", \"text\", \"num_pages\"])\n",
        "for filename in os.listdir():\n",
        "    if filename.endswith('.pdf'):\n",
        "        doc = fitz.open(filename)\n",
        "        num_pages = doc.pageCount  \n",
        "        doc_df = pd.DataFrame({\"filename\":filename, \n",
        "                               \"text\":\"\", \n",
        "                               \"num_pages\":num_pages}, index=[0])\n",
        "        doc_text = \"\"\n",
        "        # get the text from each page in the document\n",
        "        for idx, page in enumerate(doc):\n",
        "            page = doc.loadPage(idx)\n",
        "            page_text = page.getText(\"text\")\n",
        "            doc_text = doc_text + page_text\n",
        "        # store the document text in the document dataframe\n",
        "        doc_df[\"text\"] = doc_text \n",
        "        # append the document df to the folder df\n",
        "        df = df.append(doc_df)\n",
        "\n",
        "# reset the index\n",
        "df = df.reset_index(drop=True) \n",
        "\n",
        "# create lower case version of the text\n",
        "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
        "\n",
        "\n",
        "files_found_list = []\n",
        "page_nums=[]\n",
        "\n",
        "# loop through the search terms\n",
        "for search_term in all_search_terms:\n",
        "    # make a note of files that contain the search term\n",
        "    files_found_list.append(list(df.loc[df[\"text_lower\"].str.contains(search_term), \"filename\"]))\n",
        "\n",
        "# flatten the list of lists to one list\n",
        "files_found_list = [item for sublist in files_found_list for item in sublist]\n",
        "\n",
        "# if there was at least one file containing one of the search terms\n",
        "if len(files_found_list) > 0:\n",
        "    file_df = pd.DataFrame(columns=[\"filename\", \"keyword\", \"page\"], index=[0])\n",
        "    \n",
        "    # loop through the files containing the keyword and get the list of page numbers containing the keyword\n",
        "    for file in files_found_list:\n",
        "        filename = file\n",
        "        pdf_document = fitz.open(filename)\n",
        "        num_pages_in_file = len(pdf_document)\n",
        "        \n",
        "        for current_page in range(len(pdf_document)):\n",
        "            page = pdf_document.loadPage(current_page)\n",
        "            page_text = page.getText(\"text\")\n",
        "            page_text = page_text.lower()\n",
        "            \n",
        "            for search_term in all_search_terms:\n",
        "                if page.searchFor(search_term):\n",
        "                    page_nums.append(current_page+1)\n",
        "                    page_keyword_df = pd.DataFrame({\"filename\":filename, \n",
        "                                                    \"keyword\":search_term,\n",
        "                                                    \"page\":current_page+1}, \n",
        "                                                   index=[0])\n",
        "                    file_df = file_df.append(page_keyword_df)\n",
        "                \n",
        "else:\n",
        "    print(\"Search terms not found in the documents provided.\")\n",
        "\n",
        "file_df = file_df.dropna()\n",
        "file_df = file_df.drop_duplicates(keep='first')\n",
        "file_df = file_df.reset_index(drop=True)\n",
        "file_df = file_df.sort_values(by=[\"filename\", \"keyword\", \"page\"])\n",
        "file_df.to_csv('file_df.csv')\n",
        "\n",
        "# get number of \"hits\" (keyword-pages) per file \n",
        "hits_per_file = pd.DataFrame(file_df.groupby(\"filename\")[\"keyword\"].count()).reset_index()\n",
        "hits_per_file = hits_per_file.rename(columns={\"keyword\":\"hits\"})\n",
        "hits_per_file = hits_per_file.sort_values(by=[\"hits\", \"filename\"], ascending=False)\n",
        "\n",
        "# add number of pages as a column\n",
        "hits_per_file = hits_per_file.merge(df[[\"filename\", \"num_pages\"]])\n",
        "\n",
        "# add hits/pages as a column\n",
        "hits_per_file[\"hits/pages ratio\"] = hits_per_file[\"hits\"] / hits_per_file[\"num_pages\"]\n",
        "\n",
        "# save to a csv\n",
        "hits_per_file.to_csv('hits_per_file.csv')\n",
        "\n",
        "# get unique keywords per file\n",
        "keywords_by_file = file_df[[\"filename\", \"keyword\"]].drop_duplicates(keep='first')\n",
        "keywords_by_file.to_csv('keywords_by_file.csv')\n",
        "\n",
        "# get relevant pages per file\n",
        "pages_by_file = file_df[[\"filename\", \"page\"]].drop_duplicates(keep='first')\n",
        "pages_by_file.to_csv('pages_by_file.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}