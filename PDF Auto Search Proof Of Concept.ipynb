{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PDF Auto Search Proof of Concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:56.962340Z",
     "start_time": "2020-11-05T15:58:56.957615Z"
    }
   },
   "outputs": [],
   "source": [
    "#conda install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:56.989212Z",
     "start_time": "2020-11-05T15:58:56.968887Z"
    }
   },
   "outputs": [],
   "source": [
    "#import fitz  # from PyMuPDF - for converting PDFs to text data\n",
    "import pandas as pd  # for dataframe manipulation\n",
    "import spacy # for finding similar words\n",
    "import os  # for getting list of files in a directory and looking only at PDFs\n",
    "import nltk  # for stemming the keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:57.004392Z",
     "start_time": "2020-11-05T15:58:56.994171Z"
    }
   },
   "outputs": [],
   "source": [
    "# what files are in the working directory?\n",
    "list_of_filenames = os.listdir()\n",
    "list_of_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop Through PDFs in Directory and Read Text into Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.646808Z",
     "start_time": "2020-11-05T15:58:57.010469Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# for each pdf, read in the text to a new row in a dataframe\n",
    "df = pd.DataFrame(columns=[\"filename\", \"text\", \"num_pages\"])\n",
    "for filename in os.listdir():\n",
    "    if filename.endswith('.pdf'):\n",
    "        doc = fitz.open(filename)\n",
    "        num_pages = doc.pageCount  \n",
    "        doc_df = pd.DataFrame({\"filename\":filename, \n",
    "                               \"text\":\"\", \n",
    "                               \"num_pages\":num_pages}, index=[0])\n",
    "        doc_text = \"\"\n",
    "        # get the text from each page in the document\n",
    "        for idx, page in enumerate(doc):\n",
    "            page = doc.loadPage(idx)\n",
    "            page_text = page.getText(\"text\")\n",
    "            doc_text = doc_text + page_text\n",
    "        # store the document text in the document dataframe\n",
    "        doc_df[\"text\"] = doc_text \n",
    "        # append the document df to the folder df\n",
    "        df = df.append(doc_df)\n",
    "\n",
    "# reset the index\n",
    "df = df.reset_index(drop=True) \n",
    "\n",
    "# create lower case version of the text\n",
    "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
    "\n",
    "# view first 2 rows of dataframe showing the text for each file\n",
    "df.head(2).style.set_properties(subset=['text','text_lower'], **{'width': '400px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Keywords to Search For (In Lower Case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.651883Z",
     "start_time": "2020-11-05T15:58:58.648998Z"
    }
   },
   "outputs": [],
   "source": [
    "search_terms = [\"internet\", \"technology\", \"access\", \"mobile phone\", \"coverage\", \"smartphone\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stem Keywords\n",
    "\n",
    "Get the stemmed versions of the keywords provided and **add** them to the keywords list.\n",
    "\n",
    "e.g. change 'enrolment' to 'enrol' so that all the versions of the word can be found e.g. enrolled\n",
    "\n",
    "**Note**: the stemmed versions will be added to the keywords list rather than replacing the keywords lkist because sometimes the stemmed version excludes the original search term. For example, using the Porter stemming method, 'literacy' would be stemmed to 'literaci' and therefore 'literacy' would not be identified in the documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.663768Z",
     "start_time": "2020-11-05T15:58:58.653934Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#call the nltk downloader and download the 'punkt' model\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.675068Z",
     "start_time": "2020-11-05T15:58:58.669351Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the package for doing stemming using the Porter method\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "#create an object of class PorterStemmer\n",
    "porter = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.695456Z",
     "start_time": "2020-11-05T15:58:58.686431Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to read in a list of words, \n",
    "# loop through each word, and for each word\n",
    "# obtain the stemmed version.\n",
    "\n",
    "def get_stemmed_words(list_of_search_terms):\n",
    "    \n",
    "    stemmed_search_terms = []\n",
    "    \n",
    "    for word in list_of_search_terms:\n",
    "        \n",
    "        # get stemmed version of the word\n",
    "        stemmed_word = porter.stem(word)\n",
    "        \n",
    "        # add the stemmed version to the list of search terms\n",
    "        stemmed_search_terms.append(stemmed_word)\n",
    "    \n",
    "    return stemmed_search_terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.708948Z",
     "start_time": "2020-11-05T15:58:58.700883Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the original list of search terms\n",
    "print(\"Original search terms:\", search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:58:58.722631Z",
     "start_time": "2020-11-05T15:58:58.715116Z"
    }
   },
   "outputs": [],
   "source": [
    "# call the function on the list of search terms \n",
    "stemmed_terms = get_stemmed_words(search_terms)\n",
    "\n",
    "# view the stemmed list of search terms\n",
    "print(\"Stemmed search terms:\", stemmed_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Similar Words\n",
    "\n",
    "e.g. keyword = \"education\", similar words = school, literacy etc?\n",
    "\n",
    "We could use this functionality to add on / suggest keywords for the search in addition to the keywords supplied by the Social Scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:59:19.712524Z",
     "start_time": "2020-11-05T15:58:58.728915Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a spacy object based on the English model\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:59:19.746221Z",
     "start_time": "2020-11-05T15:59:19.714747Z"
    }
   },
   "outputs": [],
   "source": [
    "# define a function to get the x most similar words to a word\n",
    "def most_similar(word, topn=2):\n",
    "    word = nlp.vocab[str(word)]\n",
    "    queries = [\n",
    "        w for w in word.vocab \n",
    "        if w.is_lower == word.is_lower and w.prob >= -15 and np.count_nonzero(w.vector)\n",
    "    ]\n",
    "\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return [(w.lower_,w.similarity(word)) for w in by_similarity[:topn+1] if w.lower_ != word.lower_]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:59:37.284291Z",
     "start_time": "2020-11-05T15:59:19.780354Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create function to receive a list of words and return the \n",
    "# top 2 similar words for each word in the list\n",
    "\n",
    "def get_similar_words(list_of_words):\n",
    "    \n",
    "    all_similar_words = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        spacy_word = nlp.vocab[str(word)]\n",
    "        if spacy_word.has_vector:\n",
    "        \n",
    "            # find similar words to the word, and store them in a dataframe along with their scores\n",
    "            similar_words = pd.DataFrame(most_similar(word, topn=2), columns=[\"word\", \"similarity_score\"])\n",
    "\n",
    "            # save the list of similar words\n",
    "            similar_words_list = list(similar_words[\"word\"])\n",
    "\n",
    "            # append the list of similar words to the list to be returned\n",
    "            all_similar_words.append(similar_words_list)\n",
    "        \n",
    "    # flatten the list of lists to one list\n",
    "    all_similar_words = [item for sublist in all_similar_words for item in sublist]\n",
    "    \n",
    "    # remove duplicates from the list\n",
    "    all_similar_words = list(dict.fromkeys(all_similar_words))\n",
    "    \n",
    "    # sort list in alphabetical order\n",
    "    all_similar_words.sort()\n",
    "\n",
    "    return all_similar_words\n",
    "\n",
    "\n",
    "# test the function\n",
    "get_similar_words(search_terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UK and US Spellings\n",
    "\n",
    "We could auto-find alternative spellings e.g. if Social Scientist searched for 'Labor' we could also add 'Labour' as a search term.\n",
    "\n",
    "UK and US spelling pairs are available here: http://www.tysto.com/uk-us-spelling-list.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:59:37.305822Z",
     "start_time": "2020-11-05T15:59:37.286854Z"
    }
   },
   "outputs": [],
   "source": [
    "uk_us_spelling_pairs = pd.read_csv('uk_us_spellings.csv')\n",
    "uk_us_spelling_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T15:59:37.375476Z",
     "start_time": "2020-11-05T15:59:37.325482Z"
    }
   },
   "outputs": [],
   "source": [
    "# create function to receive a list of words and return the \n",
    "# alternative spellings if there are any\n",
    "def get_alternative_spellings(list_of_words):\n",
    "    alt_spellings = []\n",
    "    \n",
    "    for word in list_of_words:\n",
    "        # if word is in the uk_spelling list\n",
    "        # then save the us_spelling\n",
    "        temp_list = list(uk_us_spelling_pairs.loc[uk_us_spelling_pairs[\"uk_spelling\"]==word, \"us_spelling\"])\n",
    "        if len(temp_list)>0:\n",
    "            alt_spellings.append(temp_list)\n",
    "        \n",
    "        # if the word is in the us_spelling list\n",
    "        # then save the uk_spelling\n",
    "        temp_list = list(uk_us_spelling_pairs.loc[uk_us_spelling_pairs[\"us_spelling\"]==word, \"uk_spelling\"])\n",
    "        if len(temp_list)>0:\n",
    "            alt_spellings.append(temp_list)\n",
    "            \n",
    "    # flatten list\n",
    "    alt_spellings = [item for sublist in alt_spellings for item in sublist]\n",
    "    \n",
    "    # remove duplicates from the list\n",
    "    alt_spellings = list(dict.fromkeys(alt_spellings))\n",
    "    \n",
    "    # sort list in alphabetical order\n",
    "    alt_spellings.sort()\n",
    "    \n",
    "    return alt_spellings\n",
    "\n",
    "# test the function\n",
    "get_alternative_spellings(search_terms + [\"color\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Entities Per File\n",
    "\n",
    "- organisations\n",
    "- money\n",
    "- dates\n",
    "- people\n",
    "- URLs\n",
    "- References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:01:00.674875Z",
     "start_time": "2020-11-05T15:59:37.381922Z"
    }
   },
   "outputs": [],
   "source": [
    "extracted_entities = pd.DataFrame(columns=[\"filename\", \"entity\", \"entity_type\"], index=[0])\n",
    "\n",
    "filenames = df[\"filename\"]\n",
    "texts = df[\"text_lower\"]\n",
    "\n",
    "for idx, filename in enumerate(filenames):\n",
    "    \n",
    "    text = texts[idx]\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    for ent in doc.ents: \n",
    "        temp_df = pd.DataFrame({\"filename\":filename, \"entity\":ent.text, \"entity_type\":ent.label_}, index=[0])\n",
    "        extracted_entities = extracted_entities.append(temp_df, ignore_index=True)\n",
    "\n",
    "# remove rows with no values\n",
    "extracted_entities = extracted_entities.dropna()\n",
    "\n",
    "# remove duplicates\n",
    "extracted_entities = extracted_entities.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:01:00.713127Z",
     "start_time": "2020-11-05T16:01:00.679248Z"
    }
   },
   "outputs": [],
   "source": [
    "print(extracted_entities.shape)\n",
    "print(extracted_entities.groupby(\"filename\").size())\n",
    "print(extracted_entities.groupby(\"entity_type\").size())\n",
    "extracted_entities.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:01:00.777005Z",
     "start_time": "2020-11-05T16:01:00.717039Z"
    }
   },
   "outputs": [],
   "source": [
    "# save the extracted entities to a csv\n",
    "extracted_entities.to_csv('extracted_entities.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** - this entity extraction is far from perfect. Would be useful to look into ways to improve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataframe of Files, Pages and Keywords Found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:01:18.023013Z",
     "start_time": "2020-11-05T16:01:00.791877Z"
    }
   },
   "outputs": [],
   "source": [
    "# find similar words to the search terms and add them to the list of search terms\n",
    "all_search_terms = search_terms + get_similar_words(search_terms)\n",
    "\n",
    "# find alternative spellings of search terms and add them to the list\n",
    "all_search_terms = all_search_terms + get_alternative_spellings(all_search_terms)\n",
    "\n",
    "# stem the list of search terms and add them to the list\n",
    "all_search_terms = all_search_terms + get_stemmed_words(all_search_terms)\n",
    "\n",
    "# remove duplicates from the list of search terms\n",
    "all_search_terms = list(dict.fromkeys(all_search_terms))\n",
    "\n",
    "# sort the list of search terms in alphabetical order\n",
    "all_search_terms.sort()\n",
    "\n",
    "# view the updated list of search terms\n",
    "print(\"Updated search terms list:\", all_search_terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:01:18.032560Z",
     "start_time": "2020-11-05T16:01:18.028099Z"
    }
   },
   "outputs": [],
   "source": [
    "# edit updated keyword list to remove irrelevant words and add any other relevant words manually\n",
    "all_search_terms = ['internet', \n",
    "                    'technology', 'technolog',\n",
    "                    'access', \n",
    "                    'mobile phone', \n",
    "                    'coverag', 'coverage', \n",
    "                    'smartphone', 'smart phone', 'handset', 'hand-set', 'hand set', 'cell phone', 'cellphone', \n",
    "                    'online', 'web', 'wifi', 'wi-fi','comput', 'digital', 'laptop', 'connect']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:04:23.383375Z",
     "start_time": "2020-11-05T16:01:18.035710Z"
    }
   },
   "outputs": [],
   "source": [
    "files_found_list = []\n",
    "page_nums=[]\n",
    "\n",
    "# loop through the search terms\n",
    "for search_term in all_search_terms:\n",
    "    # make a note of files that contain the search term\n",
    "    files_found_list.append(list(df.loc[df[\"text_lower\"].str.contains(search_term), \"filename\"]))\n",
    "\n",
    "# flatten the list of lists to one list\n",
    "files_found_list = [item for sublist in files_found_list for item in sublist]\n",
    "\n",
    "# if there was at least one file containing one of the search terms\n",
    "if len(files_found_list) > 0:\n",
    "    file_df = pd.DataFrame(columns=[\"filename\", \"keyword\", \"page\"], index=[0])\n",
    "    \n",
    "    # loop through the files containing the keyword and get the list of page numbers containing the keyword\n",
    "    for file in files_found_list:\n",
    "        filename = file\n",
    "        pdf_document = fitz.open(filename)\n",
    "        num_pages_in_file = len(pdf_document)\n",
    "        \n",
    "        for current_page in range(len(pdf_document)):\n",
    "            page = pdf_document.loadPage(current_page)\n",
    "            page_text = page.getText(\"text\")\n",
    "            page_text = page_text.lower()\n",
    "            \n",
    "            for search_term in all_search_terms:\n",
    "                if page.searchFor(search_term):\n",
    "                    page_nums.append(current_page+1)\n",
    "                    page_keyword_df = pd.DataFrame({\"filename\":filename, \n",
    "                                                    \"keyword\":search_term,\n",
    "                                                    \"page\":current_page+1}, \n",
    "                                                   index=[0])\n",
    "                    file_df = file_df.append(page_keyword_df)\n",
    "                \n",
    "else:\n",
    "    print(\"Search terms not found in the documents provided.\")\n",
    "\n",
    "file_df = file_df.dropna()\n",
    "file_df = file_df.drop_duplicates(keep='first')\n",
    "file_df = file_df.reset_index(drop=True)\n",
    "file_df = file_df.sort_values(by=[\"filename\", \"keyword\", \"page\"])\n",
    "file_df.to_csv('file_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:04:23.421254Z",
     "start_time": "2020-11-05T16:04:23.386697Z"
    }
   },
   "outputs": [],
   "source": [
    "# view first 10 rows of pages per keyword per file\n",
    "file_df.head(10).style.set_properties(subset=['filename'], **{'width': '600px'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Note\n",
    "\n",
    "For 2 initial search words, and **10** similar words, this took **10 mins** for 3 MICS reports (~300 to ~500 pages each) plus one 50 page document, one 3 page document, and 3 documents with 1 page each. i.e. **approx 1200 pages**.\n",
    "\n",
    "For 2 initial search words, and **5** similar words, and stemming, this took **8 mins 51 s** for the same reports as above.\n",
    "\n",
    "For 2 initial search words, and **3** similar words, and stemming, this took **6 mins 27 s** for the same reports as above\n",
    "\n",
    "For 2 initial search words, and **2** similar words, and stemming, this took **5 mins 8 s** for the same reports as above\n",
    "\n",
    "For **3** initial search words, and **2** similar words, and stemming, this took **9 mins 13 s** for the same reports as above.\n",
    "\n",
    "**Outstanding Question:** How could we optimise the code so that it runs faster?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:04:23.640612Z",
     "start_time": "2020-11-05T16:04:23.434516Z"
    }
   },
   "outputs": [],
   "source": [
    "# get number of \"hits\" (keyword-pages) per file \n",
    "hits_per_file = pd.DataFrame(file_df.groupby(\"filename\")[\"keyword\"].count()).reset_index()\n",
    "hits_per_file = hits_per_file.rename(columns={\"keyword\":\"hits\"})\n",
    "hits_per_file = hits_per_file.sort_values(by=[\"hits\", \"filename\"], ascending=False)\n",
    "\n",
    "# add number of pages as a column\n",
    "hits_per_file = hits_per_file.merge(df[[\"filename\", \"num_pages\"]])\n",
    "\n",
    "# add hits/pages as a column\n",
    "hits_per_file[\"hits/pages ratio\"] = hits_per_file[\"hits\"] / hits_per_file[\"num_pages\"]\n",
    "\n",
    "# save to a csv\n",
    "hits_per_file.to_csv('hits_per_file.csv')\n",
    "\n",
    "# view the hits per file\n",
    "hits_per_file.style.set_properties(subset=['filename'], **{'width': '600px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:04:23.801965Z",
     "start_time": "2020-11-05T16:04:23.649119Z"
    }
   },
   "outputs": [],
   "source": [
    "# get unique keywords per file\n",
    "keywords_by_file = file_df[[\"filename\", \"keyword\"]].drop_duplicates(keep='first')\n",
    "keywords_by_file.to_csv('keywords_by_file.csv')\n",
    "\n",
    "# view the keyword per file\n",
    "keywords_by_file.style.set_properties(subset=['filename'], **{'width': '600px'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:04:23.848321Z",
     "start_time": "2020-11-05T16:04:23.803865Z"
    }
   },
   "outputs": [],
   "source": [
    "# get relevant pages per file\n",
    "pages_by_file = file_df[[\"filename\", \"page\"]].drop_duplicates(keep='first')\n",
    "pages_by_file.to_csv('pages_by_file.csv')\n",
    "\n",
    "# view the relevant pages per file (first 10 rows)\n",
    "pages_by_file.head(10).style.set_properties(subset=['filename'], **{'width': '600px'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
